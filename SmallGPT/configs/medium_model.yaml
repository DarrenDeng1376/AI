# Medium model configuration
model:
  vocab_size: 15000
  embed_dim: 768
  num_heads: 12
  num_layers: 8
  max_seq_len: 1024
  dropout: 0.1
  pad_token_id: 0

training:
  batch_size: 24
  learning_rate: 3e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  max_steps: 50000
  warmup_steps: 2000
  
  optimizer: adamw
  lr_scheduler: linear_warmup
  min_lr: 1e-6
  grad_clip_norm: 1.0
  gradient_accumulation_steps: 2
  
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000
  
  output_dir: ./models/medium
  data_dir: ./data
  max_seq_len: 1024
  num_workers: 4
  
  use_wandb: true
  wandb_project: smallgpt
  experiment_name: medium_model
  use_amp: true

tokenizer:
  vocab_size: 15000
  min_frequency: 2
  num_merges: 14000
  special_tokens:
    pad_token: "<pad>"
    unk_token: "<unk>"
    bos_token: "<bos>"
    eos_token: "<eos>"

generation:
  max_length: 200
  temperature: 0.8
  top_k: 50
  top_p: 0.9
  do_sample: true
  num_return_sequences: 1
  repetition_penalty: 1.1
  length_penalty: 1.0
  stop_tokens: []
