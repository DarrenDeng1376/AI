# Small model configuration for quick experimentation
model:
  vocab_size: 5000
  embed_dim: 256
  num_heads: 4
  num_layers: 4
  max_seq_len: 512
  dropout: 0.1
  pad_token_id: 0

training:
  batch_size: 16
  learning_rate: 5e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  max_steps: 10000
  warmup_steps: 500
  
  optimizer: adamw
  lr_scheduler: linear_warmup
  min_lr: 0.0
  grad_clip_norm: 1.0
  gradient_accumulation_steps: 1
  
  log_interval: 100
  eval_interval: 1000
  save_interval: 2000
  
  output_dir: ./models/small
  data_dir: ./data
  max_seq_len: 512
  num_workers: 2
  
  use_wandb: false
  wandb_project: smallgpt
  experiment_name: small_model
  use_amp: false

tokenizer:
  vocab_size: 5000
  min_frequency: 2
  num_merges: 4500
  special_tokens:
    pad_token: "<pad>"
    unk_token: "<unk>"
    bos_token: "<bos>"
    eos_token: "<eos>"

generation:
  max_length: 100
  temperature: 0.8
  top_k: 50
  top_p: 0.9
  do_sample: true
  num_return_sequences: 1
  repetition_penalty: 1.0
  length_penalty: 1.0
  stop_tokens: []
